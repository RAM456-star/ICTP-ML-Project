#Main code that can be used as .py. Preferentially run on GPUs

import numpy as np
import tensorflow as tf
import tensorflow_probability as tfp


def data_generate(d, k, alpha, Delta, sig, v):
    c_k, c_d = 1/np.sqrt(k), 1/np.sqrt(d)  
    n = int(alpha*d**2)
    W0 = tf.random.normal((k, d), dtype=tf.float32)
    X = tf.random.normal((d, n), dtype=tf.float32)
    Z = tf.random.normal((n,), dtype=tf.float32)
    M = W0@X*c_d
    M = sig(M)        
    Y = tf.tensordot(v, M, axes=1)*c_k + np.sqrt(Delta)*Z
    return W0, X, Y, v

def H(W, X, Y, v, Delta, sig):
    k, d = W.shape
    c_k, c_d = 1/np.sqrt(k), 1/np.sqrt(d)
    M = W@X*c_d
    M = sig(M)
    D = tf.tensordot(v, M, axes=1)*c_k - Y
    H0 = -tf.reduce_sum(W**2)/2
    H1 = -tf.reduce_sum(D**2)/(2*Delta)
    return H0+H1

def H_vec(W_vec, X, Y, v, Delta, sig, k, d):
    W = tf.reshape(W_vec, (k, d))
    return H(W, X, Y, v, Delta, sig) 

def test_error(Ws, W0, v, sig, ntest):
    k, d = W0.shape
    c_k, c_d = 1/np.sqrt(k), 1/np.sqrt(d)
    Xtest = tf.random.normal((d, ntest), dtype=tf.float32)
    M = W0@Xtest*c_d
    M = sig(M)
    Ytest = tf.tensordot(v, M, axes=1)*c_k
    errors=[]
    for i in range(Ws.shape[0]):
        M = Ws[i]@Xtest*c_d
        M = sig(M)
        Yhat = tf.einsum('a,ab->b', v, M)*c_k
        error = tf.reduce_mean((Yhat - Ytest)**2)/2
        error = error.numpy()
        errors.append(error)
    return errors

def q2(Ws, W0, v):
    k, d = W0.shape
    S0 = tf.einsum('k,ki,kj->ij', v, W0, W0)
    Ss = tf.einsum('k,nki,nkj->nij', v, Ws, Ws)
    q2s = 1 - tf.reduce_sum((Ss - S0)**2/(2*k*d**2), axis=[1, 2])
    return q2s.numpy()

def overlap(Ws, W0, v, order):
    k, d = W0.shape
    q00 = tf.squeeze(v[None, :]@tf.pow(W0@tf.transpose(W0), order)@v[:, None]).numpy()/(k*d**order)
    ovs = []
    for W in Ws:
        q01 = tf.squeeze(v[None, :]@tf.pow(W0@tf.transpose(W), order)@v[:, None]).numpy()/(k*d**order)
        q11 = tf.squeeze(v[None, :]@tf.pow(W@tf.transpose(W), order)@v[:, None]).numpy()/(k*d**order)
        ov = 1-(q00+q11-2*q01)/2
        ovs.append(ov)
    return ovs


def hmc(hmc_params, W_init, X, Y, v, k, alpha, Delta, sig, show_acceptance_rate, show_adaptation_steps):
    d, _ = X.shape
    step_size = hmc_params['step_size']
    num_leapfrog_steps = hmc_params['num_leapfrog_steps']
    num_adaptation_steps = hmc_params['num_adaptation_steps']
    num_post_adapt_steps = hmc_params['num_post_adapt_steps']
    
    W_init = tf.reshape(W_init, [-1])
    
    hmc_kernel = tfp.mcmc.HamiltonianMonteCarlo(
        target_log_prob_fn=lambda W_vec: H_vec(W_vec, X, Y, v, Delta, sig, k, d),
        step_size=step_size,
        num_leapfrog_steps=num_leapfrog_steps)
    
    adaptive_kernel = tfp.mcmc.SimpleStepSizeAdaptation(
        inner_kernel=hmc_kernel,
        num_adaptation_steps= num_adaptation_steps)
    
    @tf.function
    def run_chain():
        samples, kernel_results = tfp.mcmc.sample_chain(
            num_results=num_adaptation_steps + num_post_adapt_steps,
            current_state=W_init,
            num_burnin_steps=0,
            kernel=adaptive_kernel,
            trace_fn=lambda current_state, kernel_results:  kernel_results
        )
        return samples, kernel_results
        
    Ws, kernel_results = run_chain()
    if show_adaptation_steps:
        Ws = tf.concat([[W_init], Ws], axis=0)
    else:
        Ws = Ws[num_adaptation_steps:]
    Ws = tf.reshape(Ws, (tf.shape(Ws)[0], k, d))
    if show_acceptance_rate:
        acceptance_rate = tf.reduce_mean(tf.cast(kernel_results.inner_results.is_accepted, dtype=tf.float32))
        print("Acceptance rate:", acceptance_rate.numpy())
    return Ws


#This is for the posterior sampling using HMC
import numpy as np
import matplotlib.pyplot as plt

import tensorflow as tf
tf.config.set_visible_devices(tf.config.list_physical_devices('GPU')[0], 'GPU')

import hmc2

from scipy.sparse import csr_array
from scipy.sparse.csgraph import min_weight_full_bipartite_matching as match

def per(M):
    size, _ = M.shape
    per_index = match(csr_array(M), maximize=True)
    per_matrix = np.zeros((size, size), dtype=int)
    per_matrix[per_index[0], per_index[1]] = 1
    M_ = per_matrix.T @ M
    return M_

# Parameters
d = 150
k = 4
alpha_values = [0.5, 1.0, 2.0, 3.0, 4.0, 5.0, 6.0, 7.5]
Delta = 3
v = tf.constant([-3, -1, 1, 3], dtype=tf.float32)
v = v - tf.reduce_mean(v)
v /= tf.norm(v)
prior = 'gauss'

# Activation functions
sig = lambda x: (x**2 - 1)/np.sqrt(2.0) + (x**3 - 3*x)/6
# sig = tf.nn.elu
# sig = tf.nn.relu
# sig = tf.nn.tanh
# sig = lambda x: (x**2 - 1)/np.sqrt(2.0)
#sig = lambda x: (x**2 - 1)/np.sqrt(2.0) + (x**3 - 3*x)/6
# sig = lambda x: x + (x**2 - 1)/np.sqrt(2.0) + (x**3 - 3*x)/6
# sig = lambda x: (x**3 - 3*x)/np.sqrt(6)



params = {
    'step_size': 0.01,
    'num_leapfrog_steps': 10,
    'num_adaptation_steps': 1000,
    'num_post_adapt_steps': 0
}

# subplots for all alpha values

n_1 = len(alpha_values)
assert n_1 <= 8, "You can only plot up to 8 alpha values in a 4x4 grid (2 plots per alpha)."

fig, axes = plt.subplots(4, 4, figsize=(16, 12), tight_layout=True)  # 4x4 grid for visibility
axes = axes.reshape(4, 4)  # Ensure it's in row×col format

for i, alpha in enumerate(alpha_values):
    print(f"Processing α = {alpha}")
    
    # Generate data
    W0, X, Y, v = hmc2.data_generate(d, k, alpha, Delta, sig, v)
    W_ = tf.random.normal((k, d), dtype=tf.float32)
    
    # Posterior sampling
    Ws_unfo = hmc2.hmc(params, W_, X, Y, v, k, alpha, Delta, sig, show_acceptance_rate=False, show_adaptation_steps=True)
    Ws_info = hmc2.hmc(params, W0, X, Y, v, k, alpha, Delta, sig, show_acceptance_rate=False, show_adaptation_steps=True)

    # Final samples
    Wi = Ws_info[-1]
    Wu = Ws_unfo[-1]

    # Compute alignment matrices
    Si = (W0 @ tf.transpose(Wi) / d).numpy()
    Su = (W0 @ tf.transpose(Wu) / d).numpy()

    # Row and column indices in 4x4 grid
    row, col_base = divmod(i, 2)
    col = 2 * col_base

    # Plot informative
    ax1 = axes[row, col]
    im1 = ax1.imshow(Si**2, cmap='hot', aspect='auto', vmin=0, vmax=1)
    ax1.set_title(f'Info (α={alpha})')
    ax1.set_xlabel('Student')
    ax1.set_ylabel('Teacher')
    fig.colorbar(im1, ax=ax1, shrink=0.7)

    # Plot uninformative
    ax2 = axes[row, col + 1]
    im2 = ax2.imshow(Su**2, cmap='hot', aspect='auto', vmin=0, vmax=1)
    ax2.set_title(f'Unfo (α={alpha})')
    ax2.set_xlabel('Student')
    ax2.set_ylabel('Teacher')
    fig.colorbar(im2, ax=ax2, shrink=0.7)

#plt.suptitle("Teacher-Student Overlap Matrices (Informative vs Uninformative)", fontsize=18)
plt.subplots_adjust(top=0.92)
plt.savefig("Overlap_matrix.png", dpi=300, bbox_inches='tight')
plt.show()











#This is the teacher student workflow


# The alignment helper
def per(M):
    size, _ = M.shape
    per_index = match(csr_array(M), maximize=True)
    per_matrix = np.zeros((size, size), dtype=int)
    per_matrix[per_index[0], per_index[1]] = 1
    return per_matrix.T @ M

# Teacher data generator
def data_generate(d, k, alpha, Delta, sig, v):
    c_k, c_d = 1/np.sqrt(k), 1/np.sqrt(d)
    n = int(alpha * d**2)
    W0 = tf.random.normal((k, d), dtype=tf.float32)
    X = tf.random.normal((d, n), dtype=tf.float32)
    Z = tf.random.normal((n,), dtype=tf.float32)
    M = sig(W0 @ X * c_d)
    Y = tf.tensordot(v, M, axes=1) * c_k + tf.sqrt(Delta) * Z
    return W0, X, Y, v


def train_student(X, Y, k, d, v, Delta, sig, lr=1e-2, steps=1000, track_alignments=False, W0=None):
    c_k, c_d = 1/np.sqrt(k), 1/np.sqrt(d)
    W = tf.Variable(tf.random.normal((k, d), dtype=tf.float32))
    optimizer = tf.keras.optimizers.Adam(learning_rate=lr)

    snapshots = []  # To store alignment matrices

    for step in range(steps):
        with tf.GradientTape() as tape:
            M = sig(W @ X * c_d)
            Y_hat = tf.tensordot(v, M, axes=1) * c_k
            loss = tf.reduce_mean((Y_hat - Y)**2) / (2 * Delta)
        grads = tape.gradient(loss, [W])
        optimizer.apply_gradients(zip(grads, [W]))

        # Capture alignment matrix if required
        if track_alignments and step % 100 == 0:
            S = (W0 @ tf.transpose(W) / d).numpy()
            snapshots.append(S)

    if track_alignments:
        return W.numpy(), snapshots
    else:
        return W.numpy()   # M, Y_hat


# Main experiment loop
d, k = 150, 4
alpha_values = [0.5, 1.0, 2.0, 3.0, 4.0, 5.0, 6.0, 7.5]
Delta = 2.0
v = tf.constant([-3, -1, 1, 3], dtype=tf.float32)
v = v - tf.reduce_mean(v)
v /= tf.norm(v)

# Choice of activation
sig = lambda x: (x**2 - 1)/np.sqrt(2.0) + (x**3 - 3*x)/6
alignments = []

# Container for alpha = 6 alignment snapshots
alpha_6_snapshots = []

for alpha in alpha_values:
    print(f"Training for alpha = {alpha}")
    W0, X, Y, _ = data_generate(d, k, alpha, Delta, sig, v)
    W_student = train_student(X, Y, k, d, v, Delta, sig)
    
 # For alpha = 6, track evolution of alignment
    if alpha == 6:
        W_student, alpha_6_snapshots = train_student(
            X, Y, k, d, v, Delta, sig, track_alignments=True, W0=W0)
    else:
        W_student = train_student(X, Y, k, d, v, Delta, sig)

    
    S = W0 @ tf.transpose(W_student) / d
    alignments.append(S.numpy()**2)

# Plotting
fig, axes = plt.subplots(2, 4, figsize=(12, 4.5), tight_layout=True)
axes = axes.flatten()  # Flatten to 1D list for easy indexing

for i, (S, alpha) in enumerate(zip(alignments, alpha_values)):
    ax = axes[i]
    aligned = per(S)
    im = ax.imshow(aligned**2, cmap='hot', aspect='auto', vmin=0, vmax=1)
    ax.set_title(f"α={alpha}")
    plt.colorbar(im, ax=ax)

#plt.suptitle("Alignment matrices for different α", fontsize=16)
plt.savefig("alignment_matrices.png", dpi=300, bbox_inches='tight')
plt.show()




